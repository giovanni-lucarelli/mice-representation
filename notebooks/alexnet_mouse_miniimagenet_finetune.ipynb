{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning AlexNet on Mini-ImageNet with Mouse-Calibrated Transforms\n",
        "\n",
        "This notebook loads an ImageNet-pretrained AlexNet, freezes the first two convolutional blocks, and fine-tunes the remaining layers on Mini-ImageNet. Images are preprocessed using the `mouse_transform` to emulate mouse visual statistics (blur + noise + optional grayscale), as in `scripts/train.py` (img_size=224, blur_sig=1.76, noise_std=0.25).\n",
        "\n",
        "We will:\n",
        "- Import the project utilities and set up reproducibility\n",
        "- Define the mouse-calibrated `train` and `eval` transforms\n",
        "- Load Mini-ImageNet train/val/test splits with these transforms\n",
        "- Load ImageNet-pretrained AlexNet, freeze conv blocks 1–2, replace the classifier head\n",
        "- Train from conv block 3 onwards, validate, and test\n",
        "- Save the best model and visualize a batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment and setup\n",
        "We add the repository to `sys.path` for imports, set seeds for reproducibility, and pick the best available device (GPU if present).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys, os, random, time, math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add repo to import path\n",
        "repo_root = Path(\"/home/gamerio/Desktop/mousediet/mice-representation\").resolve()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "# Reproducibility\n",
        "seed = 3105\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "# cudnn: keep fast algorithms while avoiding full determinism penalty\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mouse-calibrated transforms\n",
        "We use `mouse_transform` with parameters aligned to `scripts/train.py` lines 76–78: `img_size=224`, `blur_sig=1.76`, `noise_std=0.25`. We keep channels as RGB while optionally converting luminance from grayscale weights. We also create an `UnNormalize` helper for visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.pipeline.mouse_transforms import mouse_transform, UnNormalize\n",
        "\n",
        "# Transform configuration (as in scripts/train.py lines 76–78)\n",
        "IMG_SIZE   = 224\n",
        "BLUR_SIG   = 1.76\n",
        "NOISE_STD  = 0.25\n",
        "TO_GRAY    = True\n",
        "APPLY_BLUR = True\n",
        "APPLY_NOISE= True\n",
        "\n",
        "train_transform = mouse_transform(\n",
        "    img_size    = IMG_SIZE,\n",
        "    blur_sig    = BLUR_SIG,\n",
        "    noise_std   = NOISE_STD,\n",
        "    to_gray     = TO_GRAY,\n",
        "    apply_blur  = APPLY_BLUR,\n",
        "    apply_noise = APPLY_NOISE,\n",
        "    train       = True,\n",
        "    self_supervised = False,\n",
        ")\n",
        "\n",
        "eval_transform = mouse_transform(\n",
        "    img_size    = IMG_SIZE,\n",
        "    blur_sig    = BLUR_SIG,\n",
        "    noise_std   = NOISE_STD,\n",
        "    to_gray     = TO_GRAY,\n",
        "    apply_blur  = APPLY_BLUR,\n",
        "    apply_noise = APPLY_NOISE,\n",
        "    train       = False,\n",
        ")\n",
        "\n",
        "unnorm = UnNormalize('imagenet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datasets and DataLoaders\n",
        "We load Mini-ImageNet splits and wrap them to return `(image, label)` tuples. Update `DATA_ROOT` to point to your dataset directory which contains `train/`, `val/`, and `test/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.datasets.DataManager import DataManager\n",
        "\n",
        "# TODO: set to your Mini-ImageNet root path\n",
        "DATA_ROOT = Path(os.environ.get('MINI_IMAGENET_ROOT', '/home/gamerio/Documents/archive')).resolve()\n",
        "if not DATA_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Set DATA_ROOT to your Mini-ImageNet path. Not found: {DATA_ROOT}\")\n",
        "\n",
        "# Loader config\n",
        "BATCH_SIZE  = 128\n",
        "NUM_WORKERS = min(12, os.cpu_count() or 0)\n",
        "\n",
        "# Use DataManager to create datasets and loaders with transforms\n",
        "dm = DataManager(\n",
        "    data_path=str(DATA_ROOT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    train_transform=train_transform,\n",
        "    eval_transform=eval_transform,\n",
        "    use_cuda=(device.type == 'cuda'),\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=4,\n",
        "    return_indices=False,\n",
        ")\n",
        "train_loader, val_loader, test_loader = dm.setup()\n",
        "\n",
        "# Dataset info\n",
        "num_classes = dm.num_classes\n",
        "print(f\"Detected {num_classes} classes\")\n",
        "\n",
        "len(dm.train_dataset), len(dm.val_dataset), len(dm.test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Peek at a batch\n",
        "We visualize a grid of images after the mouse-calibrated transform. We un-normalize to restore displayable colors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "def show_batch(loader, n=16):\n",
        "    model_was_training = False\n",
        "    batch = next(iter(loader))\n",
        "    imgs, labels = batch\n",
        "    # Unnormalize for display\n",
        "    with torch.no_grad():\n",
        "        imgs_disp = unnorm(imgs)\n",
        "    grid = vutils.make_grid(imgs_disp[:n], nrow=int(math.sqrt(n)), padding=2, normalize=False)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.axis('off')\n",
        "    plt.title('Mouse-transformed batch (unnormalized)')\n",
        "    plt.imshow(np.transpose(grid.numpy(), (1,2,0)))\n",
        "\n",
        "show_batch(train_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model: ImageNet-pretrained AlexNet and freezing conv blocks 1–2\n",
        "We load a pretrained AlexNet, replace the final classifier layer to match `num_classes`, and freeze parameters in the first two convolutional blocks (Conv1+ReLU+Pool and Conv2+ReLU+Pool). Fine-tuning begins from the third convolutional block onwards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models import alexnet\n",
        "\n",
        "# Load pretrained weights (newer torchvision versions use the 'weights' arg)\n",
        "try:\n",
        "    from torchvision.models import AlexNet_Weights\n",
        "    model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
        "except Exception:\n",
        "    model = alexnet(pretrained=True)\n",
        "\n",
        "# Replace classifier head to match dataset class count\n",
        "in_features = model.classifier[-1].in_features\n",
        "model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "# Freeze first two convolutional blocks (features[0:6])\n",
        "for p in model.features[:6].parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Channels-last can speed up convs on modern GPUs\n",
        "if device.type == 'cuda':\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "# Verify which parameters will be optimized\n",
        "num_frozen = sum(not p.requires_grad for p in model.parameters())\n",
        "num_trainable = sum(p.requires_grad for p in model.parameters())\n",
        "print(f\"Trainable params: {num_trainable} | Frozen params: {num_frozen}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training utilities\n",
        "Simple PyTorch training/evaluation loops with AMP mixed precision and accuracy tracking. Only parameters with `requires_grad=True` are optimized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Version-compatible autocast context manager\n",
        "# Uses torch.autocast when available (accepts device_type),\n",
        "# falls back to torch.cuda.amp.autocast otherwise.\n",
        "def autocast_cm():\n",
        "    try:\n",
        "        return torch.autocast(device_type=device.type, enabled=(device.type=='cuda'))\n",
        "    except Exception:\n",
        "        from torch.cuda.amp import autocast as _cuda_autocast\n",
        "        return _cuda_autocast(enabled=(device.type=='cuda'))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "\n",
        "def accuracy(outputs: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    preds = outputs.argmax(dim=1)\n",
        "    correct = (preds == targets).sum().item()\n",
        "    return 100.0 * correct / targets.size(0)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler):\n",
        "    model.train()\n",
        "    running_loss, running_acc, total = 0.0, 0.0, 0\n",
        "    pbar = tqdm(loader, desc='Train', leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        if device.type == 'cuda':\n",
        "            images = images.contiguous(memory_format=torch.channels_last)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast_cm():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        acc = accuracy(outputs.detach(), labels)\n",
        "        bs = labels.size(0)\n",
        "        running_loss += loss.item() * bs\n",
        "        running_acc  += acc * bs\n",
        "        total += bs\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{acc:.2f}%\"})\n",
        "    return running_loss / total, running_acc / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, desc='Val'):\n",
        "    model.eval()\n",
        "    running_loss, running_acc, total = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=desc, leave=False)\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            if device.type == 'cuda':\n",
        "                images = images.contiguous(memory_format=torch.channels_last)\n",
        "            with autocast_cm():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "            acc = accuracy(outputs, labels)\n",
        "            bs = labels.size(0)\n",
        "            running_loss += loss.item() * bs\n",
        "            running_acc  += acc * bs\n",
        "            total += bs\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{acc:.2f}%\"})\n",
        "    return running_loss / total, running_acc / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train, validate, and save best checkpoint\n",
        "We train for a few epochs, track validation accuracy, and save the best-performing weights to `artifacts/alexnet_mouse_miniimagenet_best.pth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ARTIFACTS_DIR = (repo_root / 'artifacts').resolve()\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_CKPT = ARTIFACTS_DIR / 'alexnet_mouse_miniimagenet_best.pth'\n",
        "\n",
        "EPOCHS = 15\n",
        "best_val_acc = -1.0\n",
        "history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scaler)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, desc='Val')\n",
        "    scheduler.step()\n",
        "\n",
        "    history.append({\n",
        "        'epoch': epoch+1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'lr': scheduler.get_last_lr()[0]\n",
        "    })\n",
        "\n",
        "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | Val: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({'model': model.state_dict(), 'epoch': epoch+1, 'val_acc': best_val_acc}, BEST_CKPT)\n",
        "        print(f\"Saved new best checkpoint: {BEST_CKPT} (val_acc {best_val_acc:.2f}%)\")\n",
        "\n",
        "# Plot curves\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot([h['epoch'] for h in history], [h['train_loss'] for h in history], label='train')\n",
        "plt.plot([h['epoch'] for h in history], [h['val_loss'] for h in history], label='val')\n",
        "plt.title('Loss'); plt.xlabel('epoch'); plt.legend();\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot([h['epoch'] for h in history], [h['train_acc'] for h in history], label='train')\n",
        "plt.plot([h['epoch'] for h in history], [h['val_acc'] for h in history], label='val')\n",
        "plt.title('Accuracy'); plt.xlabel('epoch'); plt.legend();\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load best checkpoint and evaluate on test set\n",
        "We load the best validation checkpoint and report test loss and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if BEST_CKPT.exists():\n",
        "    ckpt = torch.load(BEST_CKPT, map_location=device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    print(f\"Loaded best checkpoint from epoch {ckpt.get('epoch', '?')}, val_acc={ckpt.get('val_acc', float('nan')):.2f}%\")\n",
        "else:\n",
        "    print(f\"Best checkpoint not found at: {BEST_CKPT}. Using current model weights.\")\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, desc='Test')\n",
        "print(f\"Test: loss={test_loss:.4f}, acc={test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weight differences vs pretrained AlexNet\n",
        "This cell reloads a fresh ImageNet-pretrained AlexNet and compares its parameters with the current (fine-tuned) `model`. For each parameter with matching shape, it prints:\n",
        "- L2 norm of the difference\n",
        "- Relative L2 (L2 difference divided by pretrained L2)\n",
        "- Mean absolute difference\n",
        "- Max absolute difference\n",
        "\n",
        "Parameters with shape mismatches (e.g., replaced classifier head) are noted and skipped.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models import alexnet\n",
        "\n",
        "# Load a fresh pretrained baseline to compare against\n",
        "try:\n",
        "    from torchvision.models import AlexNet_Weights\n",
        "    baseline = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
        "except Exception:\n",
        "    baseline = alexnet(pretrained=True)\n",
        "\n",
        "baseline_state = baseline.state_dict()\n",
        "finetuned_state = model.state_dict()\n",
        "\n",
        "rows = []\n",
        "for name, ft_w in finetuned_state.items():\n",
        "    note = ''\n",
        "    if name not in baseline_state:\n",
        "        rows.append((name, None, None, None, None, 'missing_in_baseline'))\n",
        "        continue\n",
        "    base_w = baseline_state[name]\n",
        "    if base_w.shape != ft_w.shape:\n",
        "        rows.append((name, None, None, None, None, f'shape_mismatch {tuple(base_w.shape)} vs {tuple(ft_w.shape)}'))\n",
        "        continue\n",
        "    d = (ft_w.detach().float().cpu() - base_w.detach().float().cpu())\n",
        "    l2 = d.norm().item()\n",
        "    base_l2 = base_w.detach().float().cpu().norm().item()\n",
        "    rel = l2 / (base_l2 + 1e-12)\n",
        "    mean_abs = d.abs().mean().item()\n",
        "    max_abs = d.abs().max().item()\n",
        "    rows.append((name, l2, rel, mean_abs, max_abs, note))\n",
        "\n",
        "# Pretty print as a simple table\n",
        "hdr = f\"{'parameter':60s} {'L2':>12s} {'rel_L2':>12s} {'mean|Δ|':>12s} {'max|Δ|':>12s}  note\"\n",
        "print(hdr)\n",
        "print('-' * len(hdr))\n",
        "for name, l2, rel, mean_abs, max_abs, note in rows:\n",
        "    if l2 is None:\n",
        "        print(f\"{name:60s} {'-':>12} {'-':>12} {'-':>12} {'-':>12}  {note}\")\n",
        "    else:\n",
        "        print(f\"{name:60s} {l2:12.6f} {rel:12.6f} {mean_abs:12.6f} {max_abs:12.6f}  {note}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datadiet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
