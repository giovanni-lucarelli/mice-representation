data:
  batch_size: 256
  num_workers: 15
  train_split: 0.7
  val_split: 0.15
  split_seed: 42

train:
  self_supervised: true
  num_epochs: 200
  dropout_rate: 0.3
  early_stopping_patience: 100

  # optimizer:
  #   name: AdamW
  #   params:
  #     lr: 3e-5
  #     weight_decay: 1e-4

  optimizer:
    name: SGD
    params:
      lr: 0.03
      momentum: 0.9
      weight_decay: 1e-4

  # scheduler:
  #   name: ReduceLROnPlateau
  #   params:
  #     factor: 0.5
  #     patience: 5
  #     mode: min

  scheduler:
    name: MultiStepLR
    params:
      milestones: [120, 160, 180]
      gamma: 0.2
  
  loss:
    name: InstanceDiscriminationLoss
    params:
      mode: dynamic
      m: 16384
      gamma: 0.5
      tau: 0.1
      embedding_dim: 128
      model_output_dim: 4096
      warmup_epochs: 0
      z_const: 1260.17

  checkpoint_sub_dir: self-supervised_diet

  save_every_n: 10

diet:
  grayscale: true
  blur: true
  noise: true