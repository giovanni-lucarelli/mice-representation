data:
  batch_size: 256
  num_workers: 15
  train_split: 0.7
  val_split: 0.15
  split_seed: 42

train:
  self_supervised: true
  num_epochs: 100
  dropout_rate: 0.3
  early_stopping_patience: 15

  optimizer:
    name: AdamW
    params:
      lr: 3e-4
      weight_decay: 1e-4

  # scheduler:
  #   name: ReduceLROnPlateau
  #   params:
  #     factor: 0.5
  #     patience: 5
  #     mode: min

  scheduler:
    name: MultiStepLR
    params:
      milestones: [25, 50, 75]
      gamma: 0.2

  loss:
    name: InstanceDiscriminationLoss
    params:
      m: 4096
      gamma: 0.5
      tau: 0.07
      embedding_dim: 128
      model_output_dim: 4096
  
  checkpoint_sub_dir: self-supervised_no-diet

  save_every_n: 10

diet:
  grayscale: false
  blur: false
  noise: false