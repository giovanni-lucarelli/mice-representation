\vspace{-0.3em}

\section{Results}

Table~\ref{tab:ic_units_summary} reports the number of selected specimens and units after the Allen dataset preprocessing, and the pooled inter-animal consistency (RSA score) computed for each visual area. The mean RSA values, together with their corresponding standard errors of the mean (SEM), quantify the reliability of neural responses across animals within each region.

\begin{table}[h]
    \centering
    \begin{tabular}{lcccc}
        \hline
        \textbf{Area} & \textbf{Specimens} & \textbf{Units per Specimen} & \textbf{IC} \\
        \hline
        AL & 6 & [85, 127, 184, 91, 166, 89] & 0.648 $\pm$ 0.021 \\
        AM & 7 & [70, 94, 71, 72, 70, 74, 135] & 0.570 $\pm$ 0.030 \\
        LM & 6 & [51, 53, 74, 56, 77, 58] & 0.453 $\pm$ 0.031 \\
        PM & 5 & [65, 115, 62, 90, 54] & 0.552 $\pm$ 0.020 \\
        RL & 7 & [76, 69, 67, 68, 79, 95, 111] & 0.542 $\pm$ 0.027 \\
        V1 & 8 & [110, 102, 91, 85, 88, 93, 94, 85] & 0.592 $\pm$ 0.038 \\
        \hline
    \end{tabular}
    \caption{Number of specimen IDs, total units, units per specimen ID, and interanimal consistency score ($\pm$ standard error) for each visual area.}
    \label{tab:ic_units_summary}
\end{table}


\subsection{Data diet impact}

In \cref{fig:rsa-qualitative} is shown the RSA score for each visual areas and for each convolutional layer of AlexNet. We can see that the RSA score is higher in the case of data diet both at training and inference time. At training time means that all the ImageNet dataset have been preprocessed with our ``mouse-like'' pipeline, whereas at inference time means that the pipeline has been applyed to the same Allen Brain stimuli that the mice saw.

Specifically, form \cref{fig:rsa-vsrandom} where the untrained model (baseline) have been subtracted, we can see that, the diet affects negatively the first convolutional layer but leads to higer RSA score in the subsequent layers. Moreover we can observe that in some visual areas, namely AL, LM, RL, V1, for the last two layers the trained models provide a worse score with respect to the untrained one. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{./image/main-result.jpeg}
    \caption{RSA score, normalized to the noise ceiling (pooled interanimal consistency), for all visual areas and for all AlexNet convolutional layers. In \textbf{violet} the untrained model, in \textbf{blue} the model trained without data diet (inference without diet), in \textbf{green} the one trained with data diet (inference with diet).}
    \label{fig:rsa-qualitative}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{./image/deltas.jpeg}

    \vspace{-1em}
    \caption{Delta RSA vs untrained model. In \textbf{blue} the model trained without data diet (inference without diet), in \textbf{green} the one trained with data diet (inference with diet).}
    \label{fig:rsa-vsrandom}
\end{figure}

\paragraph{Random diet}

\cref{fig:random} illustrates the effect of a ``random diet''. Here, blur and noise variance are set to one for all images. For the untrained network, the highest RSA scores across areas and layers are obtained when \emph{no} diet is applied at inference; all diets, including the random one, reduce predictivity. This contrasts with the trained models, which instead benefit from appropriately chosen diets.

Because the random diet uses a single random configuration of blur and noise, these results should be considered preliminary. A more systematic exploration of the random diet parameter space is required to draw robust conclusions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{./image/random.jpeg}
    \vspace{-1em}
    \caption{Delta RSA vs untrained model. In lighter shades of violet are shown different data diets (at inference time): our diet, Nayebi's diet and one random diet.}
    \label{fig:random}
\end{figure}

\subsection{Decoupling training- and inference-time diets}

We next disentangle the contributions of training- and inference-time preprocessing. When AlexNet is trained on the original (non-mouse-like) ImageNet dataset but the data diet is applied only at inference time to the Allen stimuli, we obtain qualitatively similar trends. As shown in \cref{fig:inference}, applying the diet at inference consistently improves neural predictivity in all layers except \texttt{conv1}, for both the diet-trained and the standard ImageNet-trained models.

The effect is particularly pronounced for the network trained on unprocessed ImageNet: adding the diet only at inference brings its RSA scores close to those of the model trained directly on diet-processed ImageNet. This indicates that a substantial portion of the benefit can be achieved by reshaping the stimulus statistics at inference time, without necessarily re-training the model.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{./image/inference.jpeg}
    \vspace{-1em}
    \caption{Delta RSA vs untrained model. In \textbf{blue} the model trained without data diet (inference without diet), in \textbf{light blue} the model trained withoud data diet but with diet at inference, in \textbf{green} the one trained with data diet (inference with diet).}
    \label{fig:inference}
\end{figure}

\subsection{Decoupling blur and noise}

\Cref{fig:no-noise-no-blur} shows the separate contributions of Gaussian blur and Gaussian noise to the RSA score. In \texttt{conv1}, the best predictivity is obtained with a purely low-pass signal (blur only, no added noise) across all visual areas. However, this advantage decreases in deeper layers, where the full diet (blur + noise) yields higher scores.

The degradation in \texttt{conv1} when noise is added can be understood in terms of the approximate linearity of the first convolutional layer. Being the first stage of processing (with no additional pooling or non-linear transformations yet), \texttt{conv1} is well-approximated by a linear filter bank. Adding Gaussian noise to the input images is therefore effectively equivalent to adding noise directly on the layer's representations. Since this noise is independently sampled across images, increasing its variance reduces the correlation between feature patterns elicited by different stimuli. As a consequence, dissimilarities in the RDM become dominated by the noise term rather than by differences in stimulus statistics. In this way, the blur--acting as a low-pass filter and already providing a good match to the cortical responses at \texttt{conv1}--is degraded when noise is added on top.
%  A formal derivation of this argument is provided in the Appendix.

This interpretation is consistent with the behavior of the ``random diet'', defined by setting the variance of both the Gaussian blur and Gaussian noise to one. In this regime, the very large blur and noise produce highly corrupted representations, effectively destroying similarity structure both across images and across layers.

The key observation is that, in subsequent layers, adding noise to the input yields higher RSA scores. Noise that behaves as measurement noise at \texttt{conv1} is gradually averaged out across the network hierarchy. The remaining effect of the diet acts as a form of ``constructive interference'' that brings the RDMs of AlexNet closer to the biological RDMs. In other words, the noise component behaves as a \emph{biological regularizer} for the artificial network: it weakens AlexNet's ImageNet-specific ability to discriminate between images that are effectively similar from the mouse's perspective.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{./image/no-noise.jpeg}
    \caption{}
    \label{fig:no-noise-no-blur}
\end{figure}

\paragraph{Nayebi diet}

To compare our results with Nayebi et al.~\cite{nayebi}, we defined a pseudo data diet analogous to their optimal configuration. Specifically, we rescale the stimuli to $64 \times 64$ pixels and then upsample them back to $224 \times 224$ to match AlexNet's input size. This pipeline implements a strong low-pass filter, harsher than our Gaussian blur, and can be regarded as a special case of our diet without the Gaussian noise.

As shown in \cref{fig:nayebi}, the two diets exhibit very similar behavior across areas and layers, including \texttt{conv1}. Our blur-based diet achieves slightly better results in the first layer, likely because its kernel size was optimized to approximate Prusky's contrast sensitivity function (CSF) for mice.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{./image/nayebi-diet.jpeg}
    \vspace{-1em}
    \caption{Delta RSA vs untrained model. In \textbf{blue} the model trained without data diet (inference without diet), in \textbf{light blue} the model trained withoud data diet but with diet at inference, in \textbf{lighter blue} the model trained withoud data diet but with Nayebi's diet at inference, in \textbf{green} the one trained with data diet (inference with diet).}
    \label{fig:nayebi}
\end{figure}