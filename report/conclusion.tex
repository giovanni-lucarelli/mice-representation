\section{Discussion}

In this project, we developed a biologically-informed preprocessing pipeline to feed visual stimuli to artificial neural networks that more closely resemble those perceived by mice. We trained AlexNet on the transformed ImageNet dataset and evaluated its neural similarity to biological responses recorded in the Allen Neuropixels dataset. 

Our results demonstrate that the ecological data diet, \textit{i.e.}, the one obtained by our preprocessing strategy, provided both at training and inference time, provides the largest neural predictivity for all visual areas adn for all convolutional layers of AlexNet, except for the first one (see \cref{fig:rsa-vsrandom}). We note also that in all visual areas the peaks in neural predictivity are always in layers \texttt{conv2}, \texttt{conv3}. Therefore, we do not have evidence of a hierarchical structure in the mouse visual cortex, as already stated by Nayebi \textit{et al.} 

We also observed a clear decoupling effect, in the neural predictivity score, if the diet is provided at training or at inference time (see \cref{fig:inference}). While all tested data diets improved the predictivity of pretrained models, they consistently decreased the predictivity of untrained networks with random weights (see \cref{fig:random}). This indicates that the diet engages meaningfully with learned feature representions. In the case of trained models (both with diet and without) the inference time diet always improves the score, but with a clear larger impact on the model trained on unprocessed images, leading to a score almost always close to the one obtained by the model trained on preprocessed images (see \cref{fig:inference}). 

By disentangling the contributions of Gaussian blur and Gaussian noise (\cref{fig:no-noise-no-blur}) and comparing them to the Nayebi pseudo-diet (\cref{fig:nayebi}), we obtained additional insight into how ecological preprocessing shapes neural predictivity. In \texttt{conv1}, a purely low-pass signal (blur only, no noise) produced the highest RSA scores across all visual areas, while performance degraded when additive Gaussian noise was introduced. This is consistent with the approximate linearity of the first convolutional layer: \texttt{conv1} can be viewed as a linear filter acting directly on the input, so adding independent Gaussian noise to the images is effectively equivalent to adding noise to the layer's representations. Because this noise term is uncorrelated across images, it reduces correlations between activity patterns and causes dissimilarities in the RDM to be dominated by the noise rather than by stimulus-driven structure, thereby degrading a blur-only signal that already provides a good match to mouse spatial acuity. In deeper layers, however, the full ecological diet (blur + noise) consistently outperforms the blur-only condition: noise that behaves as ``measurement noise'' in \texttt{conv1} is progressively attenuated and averaged out by the hierarchical transformations (convolutions, nonlinearities, pooling), and what remains is a reshaping of the representational geometry that undermines AlexNet's ImageNet-specific reliance on high-frequency, texture-like cues and reduces distinctions between images that are effectively similar from the mouse's perspective. In this sense, blur and noise act as complementary components: the blur aligns early spatial statistics with the mouse contrast sensitivity function, while the noise acts as a \emph{biological regularizer} that pushes deeper-layer representations toward a more mouse-like similarity structure. The Nayebi pseudo-diet can be interpreted as a particular instance of this blur-only regime with a stronger effective low-pass kernel (downscale to $64\times64$ followed by upscaling to $224\times224$), which explains its strong performance in \texttt{conv1}, whereas our calibrated blur combined with noise yields superior alignment in deeper layers. The behavior of the ``random diet'' further supports this view: when blur and noise variances are set to large, uncalibrated values, the representations become dominated by stochastic distortions and lose meaningful similarity structure at all depths (\cref{fig:random}).

Our overarching goal was to identify a model of the mouse visual system that incorporates the animal's natural visual experience. For this reason, we considered the model trained and evaluated with the ecological diet as our reference point, as it consistently processes mouse-like images. Surprisingly, however, most of the improvement in neural alignment originates at inference time: even a network trained on standard high-acuity images shows substantially increased predictivity once mouse-like transformations are applied to its inputs. This suggests that inference-time preprocessing may be the primary driver of alignment with biological neural activity. Consequently, an intriguing possibility emerges: perhaps modifying only the ``artificial retina''—the transformations applied at inference—could be sufficient to repurpose a generic vision model such as AlexNet into a plausible ``virtual brain'' for different species.

Taken together, these findings highlight the central role of stimulus realism and input statistics in brain-model correspondence analyses. They also show that seemingly simple manipulations--such as calibrated blur and controlled noise--can substantially alter representational similarity, acting both as an approximation to species-specific optics and as an effective regularizer of artificial networks. Future work should explore more detailed retinal and optical models, extend these analyses to other architectures and species, and systematically characterize how different components of ecological preprocessing interact with learning dynamics. Such efforts will be crucial for disentangling the contributions of model architecture, training objective, and input statistics to the emergence of biologically plausible visual representations.
