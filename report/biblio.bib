@article{nayebi,
    doi = {10.1371/journal.pcbi.1011506},
    author = {Nayebi, Aran AND Kong, Nathan C. L. AND Zhuang, Chengxu AND Gardner, Justin L. AND Norcia, Anthony M. AND Yamins, Daniel L. K.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation},
    year = {2023},
    month = {10},
    volume = {19},
    url = {https://doi.org/10.1371/journal.pcbi.1011506},
    pages = {1-36},
    abstract = {Studies of the mouse visual system have revealed a variety of visual brain areas that are thought to support a multitude of behavioral capacities, ranging from stimulus-reward associations, to goal-directed navigation, and object-centric discriminations. However, an overall understanding of the mouse’s visual cortex, and how it supports a range of behaviors, remains unknown. Here, we take a computational approach to help address these questions, providing a high-fidelity quantitative model of mouse visual cortex and identifying key structural and functional principles underlying that model’s success. Structurally, we find that a comparatively shallow network structure with a low-resolution input is optimal for modeling mouse visual cortex. Our main finding is functional—that models trained with task-agnostic, self-supervised objective functions based on the concept of contrastive embeddings are much better matches to mouse cortex, than models trained on supervised objectives or alternative self-supervised methods. This result is very much unlike in primates where prior work showed that the two were roughly equivalent, naturally leading us to ask the question of why these self-supervised objectives are better matches than supervised ones in mouse. To this end, we show that the self-supervised, contrastive objective builds a general-purpose visual representation that enables the system to achieve better transfer on out-of-distribution visual scene understanding and reward-based navigation tasks. Our results suggest that mouse visual cortex is a low-resolution, shallow network that makes best use of the mouse’s limited resources to create a light-weight, general-purpose visual system—in contrast to the deep, high-resolution, and more categorization-dominated visual system of primates.},
    number = {10},

}

@article{rsa,
  
    AUTHOR={Kriegeskorte, Nikolaus  and Mur, Marieke  and Bandettini, Peter A.},
            
    TITLE={Representational similarity analysis - connecting the branches of systems neuroscience},
            
    JOURNAL={Frontiers in Systems Neuroscience},
            
    VOLUME={Volume 2 - 2008},

    YEAR={2008},

    URL={https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008},

    DOI={10.3389/neuro.06.004.2008},

    ISSN={1662-5137},

}

@article{PRUSKY20043411,
title = {Characterization of mouse cortical spatial vision},
journal = {Vision Research},
volume = {44},
number = {28},
pages = {3411-3418},
year = {2004},
note = {The Mouse Visual System: From Photoreceptors to Cortex},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2004.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0042698904004390},
author = {G.T. Prusky and R.M. Douglas},
keywords = {Visual cortex, Visual water task, Acuity, Contrast sensitivity, C57, Mus musculus, V1, Striate, Behavior},
abstract = {Little is known about the spatial vision of mice or of the role the visual cortex plays in mouse visual perception. In order to provide baseline information upon which to evaluate the spatial vision of experimentally and genetically altered mice, we used the visual water task to assess the contrast sensitivity and grating acuity of normal C57BL/6 mice. We then ablated striate cortex (V1) bilaterally and re-measured the same visual functions. Intact mice displayed an inverse “U”-shaped contrast sensitivity curve with a maximum sensitivity near 0.2 cycles/degree (c/d). Grating acuity, measured either by discriminating a sine-wave grating from an equiluminant gray, or vertical from horizontal sine wave gratings, was near 0.55 c/d. Grating acuity and contrast sensitivity were reduced significantly following aspiration of V1. The mouse visual system exhibits fundamental mammalian characteristics, including the feature that striate cortex is involved in processing visual information with the highest sensitivity and spatial frequency.}
}

@article{MURATORE2025101149,
title = {Unraveling the complexity of rat object vision requires a full convolutional network and beyond},
journal = {Patterns},
volume = {6},
number = {2},
pages = {101149},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101149},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924003210},
author = {Paolo Muratore and Alireza Alemi and Davide Zoccolan},
keywords = {object recognition, perceptual strategy, invariance, artificial vs. biological vision, visual perception, rodents, convolutional neural networks},
abstract = {Summary
Despite their prominence as model systems of visual functions, it remains unclear whether rodents are capable of truly advanced processing of visual information. Here, we used a convolutional neural network (CNN) to measure the computational complexity required to account for rat object vision. We found that rat ability to discriminate objects despite scaling, translation, and rotation was well accounted for by the CNN mid-level layers. However, the tolerance displayed by rats to more severe image manipulations (occlusion and reduction of objects to outlines) was achieved by the network only in the final layers. Moreover, rats deployed perceptual strategies that were more invariant than those of the CNN, as they more consistently relied on the same set of diagnostic features across transformations. These results reveal an unexpected level of sophistication of rat object vision, while reinforcing the intuition that CNNs learn solutions that only marginally match those of biological visual systems.}
}

@techreport{alleninst_wp2024,
  author       = {Allen Institute for Brain Science},
  title        = {Neuropixels Visual Coding ― White Paper v1.0},
  institution  = {Allen Institute for Brain Science},
  year         = {2024},
  type         = {White Paper},
  url          = {https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/80/75/8075a100-ca64-429a-b39a-569121b612b2/neuropixels_visual_coding_-_white_paper_v10.pdf},
  note         = {Accessed: 2025-11-07}
}